\documentclass[a4paper, 11pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{fullpage}

\begin{document}

<<env, echo=FALSE>>=
set.seed(1)
library("xtable")
library("knitr")
source("dmin.R")
opts_chunk$set(fig.align = 'center',
               tidy = FALSE)
@

<<external-code, cache=FALSE, echo=FALSE>>=
read_chunk("dmin.R")
@

\title{Scientific Programming Assignment 2}
\maketitle

\section{$D_{min}$ model}
<<dmin2d>>=
@

An implementation of the $d_{min}$ algorithm for the generation of
spatial patterns. The main functionality is in the \texttt{dmin2d}
function that takes as arguments the mean and standard deviation of
the exclusion zone as well as the number of points to be generated and
the total area of the model region $\mathbf{A}$. The function goes
through the serial process of generating the points of the model
respecting the exclusion zones at each step. The output of the
function is an $n \times 2$ matrix of $n$ or $k$ points, where $k <
n$, after which it was not possible to add any more points. Failure to
add any more points is defined as not being able to place a new point
after $kMaxTries$ specified here initially as $100$. In the case of $k$
generated points, where $k < n$, the remaining $(k-n)$ places in the
matrix are filled with $(0, 0)$.

The function \texttt{plotDmin2d} (below) accepts the output of
\texttt{dmin2d}, the $n \times 2$ matrix of points, and plots the
spatial pattern created by those points. The model region $\mathbf{A}$
is drawn as rectangle with a dotted line.

<<plotDmin2d>>=
@

\begin{figure}[!ht]
  <<testPlot, echo=FALSE, fig.width = 5, fig.height = 5>>=
    res <- dmin2d(200, 30, 5, 200, 1000, 100, 900)
    plotDmin2d(res, 200, 1000, 100, 900)
  @
\caption{Plot of the spatial pattern consisting of points generated by
  dmin2d(200, 30, 5, 200, 1000, 100, 900). The model region $\mathbf{A}$
is denoted with the dotted line.}
\label{fig:testPlot}
\end{figure}



\section{Regularity index}
Regularity index is a measure to quantitavely evaluate the regularity
of a generated spatial pattern. If $d_i$ is the distance to the
nearest neighbouring point for point $i$ then the regularity index is
defined as $RI =
\frac{mean(\{d_i\}_{i=1:n})}{\sigma(\{d_i\}_{i=1:n})}$. Following
\texttt{ri} is a
function that calculates $RI$ from a set of points as output by the
\texttt{dmin2d} function given in the previous section.

<<ri>>=
@

<<randomRi, echo=FALSE>>=
rep.res <- replicate(1000, dmin2d(n = 200, m = 0, s = 0,
                                      xlo = 0, xhi = 1000,
                                      ylo = 0, yhi = 1000),
                         simplify=FALSE)
ri.res <- sapply(rep.res, ri)
@

The theoretical expectation for the $RI$ for a set of points randomly
positioned in the model region is $1.91$. In Figure
\ref{fig:riDistribution} we can see 1000 $RI$ values created from
simulations of the random positioning of 200 points in a square $1000
\times 1000$ region. The mean of the generated RIs is
\Sexpr{mean(ri.res)} which is close to the theoretical expectation with a standard deviation of \Sexpr{sd(ri.res)}.

\begin{figure}[!ht]
  \centering
<<riTest1, dev='pdf', echo=FALSE, eval=TRUE, fig.width=4, fig.height=5>>=

hist(ri.res, xlab="RI", main='')
@

\caption{$RI$ values obtained after 1000 $d_{min}$ model simulations
  with 200 points being randomly placed in the model region.}
\label{fig:riDistribution}
\end{figure}

The 50th largest value of the $RI$ distribution is \Sexpr{sort(ri.res,
  decreasing=TRUE)[50]}. The 50th largest value is the 95th
percentile of the distributon which is a commonly used cutoff in
significance testing. It can be used for example to test if a new RI
value comes from the same population or if it is significantly
different. The 50th largest RI values can also tell us something about
the width of the distribution.

\subsection{Relationship between $n$ and $RI$ distribution}
The relationship between $n$ the number of points in the pattern and
the distribution of RIs is not immediately obvious but becomes more
apparent when we consider a theoretical model for the
simulation. Consider the model region $R$ and smaller region
inside it $C$. The probability that a random point thrown in $A$ falls
in $C$ is given by $P(C|R) = A(C) / A(R)$. If we extend that that to
$k$ points then the probability that $k$ out of $n$ points fall inside
$C$ can be modelled with a binomial distribution like so:
\begin{equation}
P(N(C)=k |n,R) = \binom{n}{k} \left(\frac{A(C)}{A(R)}\right)^k \left(1
  - \frac{A(C)}{A(R)}\right)^{n-k} \nonumber
\end{equation}
,where $N(C)$ is the number of points in $C$. RI is the mean of all
distances to nearest neighbour divided by the standard deviation. The
distance to nearest neighbour of point $i$ $d_i$ means that there are
no points in a radious $d_i$ of the point. Let $C$ be a circular area
around a point $i$ then the probability of the distance to nearest
neighbout being $d_i$ is:
$P(d_i) = P(N(C)=0 \| n, R) = \left(1 - \frac{A(C)}{A(R)}\right)^n =
\left(1-\frac{\pi d_i^2}{A(R)}\right)^n$. The cumulative distribution
function of the nearest neighbour distance then is by definition:
\begin{equation}
F_D(d) = 1 - \left(1-\frac{\pi d^2}{A(R)}\right)^n \nonumber
\end{equation}

and from that the probability density function is:
\begin{equation}
f_D(d) = \frac{2 \pi d n}{A(R)} \left(1-\frac{\pi
    d^2}{A(R)}\right)^{n-1} \nonumber
\end{equation}

The theoretical expectation and standard deviation of that distribution is given by:
\begin{equation}
E(D) = \sum_{i=1}^n \frac{2 \pi d_i^2 n}{A(R)} \left(1-\frac{\pi
    d_i^2}{A(R)}\right)^{n-1} \nonumber
\end{equation}

and the standard deviation by:
\begin{equation}
\sigma(D) = \sum_{i=1}^n f_D(d) (d_{i} - E(D))^2 \nonumber
\end{equation}

By looking at the expressions for the thoretical expectation and
standard deviation we expect both of them to decrease as $n$ increases
and so the ratio of them RI to remain constant at $1.91$. This is confirmed by
simulation experiments(see Figure \ref{fig:nRI}). The mean of the
distribution of RIs is constant around the theoretical value of $1.91$
although it comes closer to it for larger $n$. This is because of the
decrease in the standard deviation of the distances distribution as $n$ increases which makes the
standard error around the sample mean given by $\sigma / \sqrt{n}$ lower as
$n$ increases making the sample estimates of the mean more accurate
and hence the RIs more accurate.
This is also apparent from the change in the standard deviation of the
distribution of RIs as $n$ increases which can also be seen from the
previously discussed metric of the 50th largest value of the
distribution which decreases as $n$ increases.

\begin{figure}
<<nRI, echo=FALSE>>=
n <- c(50, 100, 200, 400)
ri.res <- lapply(n, function(x) {riTest(x, 1000, 1000)})
names(ri.res) <- n
par(mfrow=c(2, 2))

for (i in 1:length(ri.res)) {
    hist(ri.res[[i]], main=paste("n=", names(ri.res[i]), sep=""), xlab="RI")
}
@
\caption{Distribution of RIs for 1000 replications of the spatial
  pattern generation for different values of $n$.}
\label{fig:nRI}
\end{figure}

\subsection{Relationship between shape of model region and $RI$ distribution}
From the theoretical investigation of the model for the random process
described above we would expect that if the area of the model region
remains the same then the shape of that region would not affect the
result and the expectations for the value of $RI$. However in practise
we observe a change in the distribution of $RI$ values as we change
the shape of the model region. Keeping the area the same and varying
the length of one of the sides to get different shapes we can see that
the mean of the distribution of $RI$ values changes significantly
especially at the two extremes where we have very narrow
rectangles(Figure \ref{fig:areaTest}).

\begin{figure}[!ht]

<<areaTest, echo=FALSE, fig.width=9, fig.height=4>>=
res.ri <- testArea(area=1000*1000)
par(mfrow=c(1, 2), cex=0.75)
plotRIArea(res.ri)
plot(res.ri$ri.50, pch=19, xlab="side 1", ylab="RI(95th perc)", xaxt='n')
axis(1, at=1:11, labels=as.character(res.ri$side))
@
\caption{On the left the average RI value for the 1000 replicate of
  the simulation as a function of the length of one of the sides of
  the model region. The highest value occurs when the two sides are of
equal size. We can also see the same thing on the right where the 50th
largest value of the $RI$ distribution is plotted as function of the
length of one of the sides of the model region.}
\label{fig:areaTest}
\end{figure}

\section{Parameter estimation}
The simulation model defined previously is only dependent on the mean
and standard deviation of the exclusion zones. The goal in this
section is to solve the inverse problem of going from some simulated data to the
model that created that data which is defined by its parameters $\mu,
\sigma$. A proposed measure of fit for a specific set of parameters is
the u-score,

\begin{equation}
u_i = \left(x_i - \frac{1}{n-1} \sum_{j \neq i} x_j  \right ) \nonumber
\end{equation}

,where $x_1$ is the RI for the real data and $\{x_j\}_{2:99}$ are RIs
from simulations with the candidate pair of parameters. We can
therefore obtain a $u$ vector of 100 values. This can give as a
p-value for the fitness of the proposed parameters. If we sort the
u-vector we can take the position of $u_1$ as the p-value for the
candidate parameters. For example if $u_1$ is in 5th place then we get
a p-value of 0.05. The idea behind this being that if the RIs for the
simulated data for the proposed parameters and the RI of the real data
are very different then $u_1$ which is the difference between real RI
and the average of the simulated RIs will be high and will sit at the
start of the sorted u-vector. The further in the sorted u-vector
$u_1$ sits then the smaller the difference to the average RIs for the
proposed parameters which implies a better fit. Another option to
asses the goodness of fit for a pair of parameters is to use $u_1$
directly as a score. The smaller the value of $u_1$ the better the fit
of the parameters.


<<fitData,echo=FALSE>>=
points <- as.matrix(read.table("spa2.dat"))
dists <- knn.dist(points, 1)
n <- length(dists)
real.ri <- ri(points)
m <- mean(dists)
s <- sd(dists)
@

I then proceeded to try to infer the parameters $\mu$ and $\sigma$
from a spatial pattern consisting of 242 points. A first approximation
can be obtained by looking at the distribution of nearest neighbour
distances directly from the data. Because the exclusion zone is
taken into account when generating the points the distribution of the
exclusion zone and the distribution of nearest neighbour distances
should be similar for large enough $n$. In this case, from the data we
can see that the distribution of nearest neighbours has mean
\Sexpr{m} and standard deviation \Sexpr{s}. Using
the p-value measure discussed above this gives a
p-value=\Sexpr{scoreParamsPVal(c(m, s), n, real.ri)} which is very good.

While the p-value statistic is a good method for comparing a small set
of parameters its discontinuous nature makes it not very generic.
For a more generic approach a search strategy has to be devised for
searching the 2-d parameter space. A crude way to do it would be to
enumerate all possible combinations, evaluate a score function, either
a $u_1$ score or the p-value statistic, and choose the best
combination based on the scoring metric used. Instead I used a simple
MCMC implementation to more efficiently move in parameter space. $u_1$
is used a score metric to guide the movement in the parameter
space. The movement proceeds serially at each iteration proposing a
new combination of parameters from a normal distribution centred around the
previous combination of parameters and with an ad-hoc selected standard deviation of
3. Let the ratio of the score of the new combination to that of the
current combination be $a$. If $a > 1$ thent the proposed combination
is accepted and a jump is made and if not the candidate parameters are
accepted with a probability $a$. The accepted point across all
iterations form the Markov Chain and if we ignore the first $k$ steps,
the stationary distribution of the chain is the distribution of our
parameters. Due to computational limitations the algorithm was run for
only 200 iterations. The resulting parameter distributions can be seen
in Figure \ref{fig:mcmcDists}. The results are similar to the results
obtained from the empirical estimation with a m=, s=(median of the two
distribution, p-value=).

\begin{figure}
<<mcmc, echo=FALSE, eval=FALSE>>=
print("Hello, world")
#res <- mcmc(n, real.ri)
#par(mfrow = c(1, 2))
#hist(res[, 1], xlab="m")
#hist(res[, 2], xlab="s")
@
\caption{Distribution of parameters obtained by MCMC}
\label{fig:mcmcDists}
\end{figure}


\section{Packing density}


<<testFail, echo=FALSE, cache=TRUE>>=
n <- 10
res <- replicate(n, packDensity())
res <- rowMeans(res)
@

For parameters m=20, s=0, xlo=0, xhi=400, ylo=0, yhi400, the
\texttt{dmin2d} model simulation was carried out to investigate the
how big $n$ has to be before the model fails. Failure is defined as
previously as not being able to add a new point to the model region
after \texttt{kMaxTries}. The theoretical maximum here, the maximum
number of points that can be added, is 400 when we consider placing
the points in a square lattice. The problem of placing the points in a
square with a fixed diameter of exlcusion zone is equivalent to the
problem of packing equal-sized circles in a square. It has been proven that the hexagonal
lattice is the optimal way to place circles in the plane where the
centres of the circles are on the vertices of the hexagons with a
density, defined as the area covered by the circles over the area of
the region, of around 0.90. In this
case however we have a bounded problem so there is no closed form
solution for finding out the maximum number of circles or points in a
specific square. Instead simulations can be carried out that can come
close to the optimal solution. One such attempt to solve the circle
packing problem for different $n$ and model area sizes is
this:.
Since we are allowed to put points on the boundary we can refine the
problem to a side model area of 420. The best simulation from the
packing project gives a maximum $n$ of 492 with a density close to the
theoretical maximum, 0.876.

Several simulation were
carried out with different number for \texttt{kMaxTries}. As
\texttt{kMaxTries} increases the number of points that can be
successfully added increases until it converges for larger
\texttt{kMaxTries} to around 270 or around 55\% of the total points
that could theoretically be added(492). It should be noted that for values
of \texttt{kMaxTries} larger than 1000 there are very small changes in
the number of points that can be added(see Figure \ref{fig:packingDens}).



\begin{figure}
<<testFailPlot, echo=FALSE, cache=TRUE>>=

plot(as.numeric(names(res)), res, ylim=c(0, 540), type='l', xlab="kMaxTries", ylab="# points")
abline(h=492, lty=2)
text(30, 502, labels="max")
@

\caption{Number of points successfully generated as a function of the
  number of maximum tries allowed before stopping. The dotted line
  signifies the theoretical maximum number of points that can be generated.}
\label{fig:packingDens}
\end{figure}




\end{document}
